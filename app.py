# app.py â€” RAG PDF (FAISSâ†’Chroma fallback) + LangChain moderno + citas
import io
import hashlib
import platform
from typing import List, Tuple

import streamlit as st
import pandas as pd
from pypdf import PdfReader

# LangChain moderno (v0.2+)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import (
    create_stuff_documents_chain,
    create_map_reduce_documents_chain,
)

# Intentar FAISS; si no existe, usar Chroma
_USE_FAISS = True
try:
    from langchain_community.vectorstores import FAISS
except Exception:
    _USE_FAISS = False
    from langchain_community.vectorstores import Chroma

# Token length (mÃ¡s realista que len())
try:
    import tiktoken
except Exception:
    tiktoken = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIG UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="RAG PDF â€“ FAISS/Chroma", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ’¬ RAG â€” PDF â†’ FAISS (auto-fallback a Chroma)")
st.caption(f"Python: **{platform.python_version()}** Â· LangChain (OpenAI + {'FAISS' if _USE_FAISS else 'Chroma'})")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UTILIDADES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def file_sha1(file_bytes: bytes) -> str:
    return hashlib.sha1(file_bytes).hexdigest()

def pdf_to_pages_text(uploaded_file) -> List[Tuple[int, str]]:
    """Devuelve lista [(page_index_1, text_1), ...]"""
    reader = PdfReader(uploaded_file)
    pages = []
    for i, p in enumerate(reader.pages):
        try:
            txt = p.extract_text() or ""
        except Exception:
            txt = ""
        pages.append((i, txt.strip()))
    return pages

def token_len(s: str) -> int:
    if not tiktoken:
        return len(s)
    enc = tiktoken.get_encoding("cl100k_base")
    return len(enc.encode(s))

def make_documents(pages: List[Tuple[int, str]], source_name: str) -> List[Document]:
    docs = []
    for i, text in pages:
        if not text:
            continue
        meta = {"source": source_name, "page": i + 1}
        docs.append(Document(page_content=text, metadata=meta))
    return docs

@st.cache_resource(show_spinner=False)
def get_embeddings(model_name: str, api_key: str):
    return OpenAIEmbeddings(model=model_name, api_key=api_key)

@st.cache_resource(show_spinner=False)
def build_vector_store(
    docs: List[Document],
    embed_model: str,
    api_key: str,
    chunk_size: int,
    chunk_overlap: int,
):
    """
    Chunkea docs y construye FAISS o Chroma (segÃºn disponibilidad).
    Cacheado para evitar recomputar en cada rerun.
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=token_len if tiktoken else len,
        separators=["\n\n", "\n", " ", ""],
    )
    chunks = splitter.split_documents(docs)
    embeddings = get_embeddings(embed_model, api_key)

    if _USE_FAISS:
        store = FAISS.from_documents(chunks, embeddings)
        store_name = "FAISS"
    else:
        # Persistencia liviana en /tmp para Streamlit Cloud
        store = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory="/tmp/chroma_rag_pdf"
        )
        store_name = "Chroma"

    return store, chunks, store_name

def annotate_citations(answer: str, docs: List[Document]) -> str:
    """Agrega [n] al final y lista de fuentes numeradas."""
    if not docs:
        return answer
    lines = ["", "##### Fuentes:"]
    for idx, d in enumerate(docs, 1):
        src = d.metadata.get("source", "PDF")
        pg = d.metadata.get("page", "?")
        preview = (d.page_content[:180] + "â€¦") if len(d.page_content) > 180 else d.page_content
        lines.append(f"[{idx}] {src} â€” pÃ¡g. {pg}\n> {preview}")
    return answer + "\n\n" + "\n\n".join(lines)

def guardrail_strict_mode(retrieved: List[Document], min_chars: int = 120) -> bool:
    """True si vale la pena responder (hay contexto suficiente)."""
    joined = " ".join(d.page_content for d in retrieved)
    return len(joined.strip()) >= min_chars

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SIDEBAR â€” opciones
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.header("âš™ï¸ ConfiguraciÃ³n")

    # API key (st.secrets preferido)
    api_key = st.text_input("OpenAI API Key", type="password", placeholder="sk-â€¦")
    if not api_key and "OPENAI_API_KEY" in st.secrets:
        api_key = st.secrets["OPENAI_API_KEY"]

    st.subheader("Modelos")
    llm_model = st.selectbox("Modelo LLM", ["gpt-4o", "gpt-4o-mini"], index=1)
    embed_model = st.selectbox(
        "Embeddings",
        ["text-embedding-3-small", "text-embedding-3-large"],
        index=0,
        help="small = +rÃ¡pido / barato; large = +calidad",
    )

    st.subheader("Chunking")
    chunk_size = st.slider("TamaÃ±o de chunk", 256, 2000, 900, step=64)
    chunk_overlap = st.slider("Solapamiento", 0, 400, 120, step=16)

    st.subheader("BÃºsqueda")
    k = st.slider("Top-K (documentos)", 1, 12, 4)
    use_mmr = st.toggle("Usar MMR (diversidad)", value=True)
    lambda_mult = st.slider("MMR lambda (relevanciaâ†”diversidad)", 0.0, 1.0, 0.5, 0.05)

    st.subheader("GeneraciÃ³n")
    temperature = st.slider("Temperatura", 0.0, 1.2, 0.1, 0.1)
    chain_type = st.selectbox("Chain Type", ["stuff", "map_reduce"], index=0)
    strict_mode = st.toggle("Modo estricto (responder solo con contexto)", value=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SUBIDA DE PDF
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("### ğŸ“„ Carga tu PDF")
uploaded = st.file_uploader("Selecciona un PDF", type=["pdf"])

if uploaded is None:
    st.info("Sube un PDF para construir el Ã­ndice y hacer preguntas.")
    st.stop()

if not api_key:
    st.warning("Ingresa tu OpenAI API key en la barra lateral o en st.secrets.")
    st.stop()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PROCESAMIENTO DEL PDF â†’ VECTOR STORE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
file_bytes = uploaded.read()
source_name = uploaded.name
file_hash = file_sha1(file_bytes)

with st.spinner("Extrayendo texto del PDFâ€¦"):
    pages = pdf_to_pages_text(io.BytesIO(file_bytes))

total_chars = sum(len(t) for _, t in pages)
valid_pages = sum(1 for _, t in pages if t)
st.success(f"âœ… PÃ¡ginas: {len(pages)} Â· PÃ¡ginas con texto: {valid_pages} Â· Caracteres: {total_chars:,}")

if valid_pages == 0:
    st.error("No se extrajo texto. Verifica que el PDF no sea escaneado/imagen. (Tip: usa OCR).")
    st.stop()

docs = make_documents(pages, source_name)

# Cacheo del Ã­ndice por hash + parÃ¡metros clave
@st.cache_resource(show_spinner=False)
def _cached_build(docs, embed_model, api_key, chunk_size, chunk_overlap):
    return build_vector_store(docs, embed_model, api_key, chunk_size, chunk_overlap)

with st.spinner("Construyendo Ã­ndice (embeddings)â€¦"):
    vectordb, chunk_docs, store_name = _cached_build(docs, embed_model, api_key, chunk_size, chunk_overlap)

st.success(f"Ãndice **{store_name}** listo con **{len(chunk_docs)}** chunks")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# INTERFAZ DE PREGUNTAS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("---")
st.subheader("â“ Pregunta al documento")

qcol, bcol = st.columns([4, 1])
question = qcol.text_area("Escribe tu pregunta", height=90, placeholder="Ej: Â¿CuÃ¡l es el objetivo principal del documento?")
ask = bcol.button("Preguntar", type="primary", use_container_width=True)

# Opciones de recuperaciÃ³n
if use_mmr:
    retriever = vectordb.as_retriever(
        search_type="mmr",
        search_kwargs={"k": k, "fetch_k": max(k * 2, 8), "lambda_mult": lambda_mult},
    )
else:
    retriever = vectordb.as_retriever(search_kwargs={"k": k})

if ask:
    if not question.strip():
        st.warning("Escribe una pregunta.")
        st.stop()

    with st.spinner("Buscando en el Ã­ndice y generando respuestaâ€¦"):
        llm = ChatOpenAI(model=llm_model, temperature=temperature, api_key=api_key)

        prompt = ChatPromptTemplate.from_messages([
            ("system",
             "Eres un asistente que responde Ãºnicamente con informaciÃ³n del contexto proporcionado. "
             "Si la respuesta no estÃ¡ en el contexto, responde de forma breve: "
             "\"No encontrÃ© esa informaciÃ³n en el documento\". "
             "Cita pÃ¡ginas al final."),
            ("human",
             "Pregunta: {input}\n\n"
             "Contexto:\n{context}")
        ])

        # Cadena de combinaciÃ³n de documentos
        if chain_type == "map_reduce":
            doc_chain = create_map_reduce_documents_chain(llm, prompt)
        else:
            doc_chain = create_stuff_documents_chain(llm, prompt)

        # 1) Recuperar documentos
        sources: List[Document] = retriever.invoke(question)

        # 2) Generar respuesta
        answer = doc_chain.invoke({"input": question, "context": sources})

        # Guardrail opcional
        if strict_mode and not guardrail_strict_mode(sources):
            st.warning("No encontrÃ© suficiente contexto relevante para responder con confianza.")
            st.stop()

        final = annotate_citations(answer, sources)

    st.markdown("### ğŸ§  Respuesta")
    st.write(final)

    with st.expander("ğŸ“š Chunks citados (fuentes)"):
        if sources:
            rows = []
            for i, d in enumerate(sources, 1):
                rows.append({
                    "#": i,
                    "pÃ¡gina": d.metadata.get("page", "?"),
                    "caracteres": len(d.page_content),
                    "preview": (d.page_content[:300] + "â€¦") if len(d.page_content) > 300 else d.page_content
                })
            df = pd.DataFrame(rows)
            st.dataframe(df, hide_index=True, use_container_width=True)
            st.download_button(
                "â¬‡ï¸ Exportar fuentes (CSV)",
                data=df.to_csv(index=False).encode("utf-8"),
                file_name="rag_fuentes.csv",
                mime="text/csv",
                use_container_width=True,
            )
        else:
            st.info("No se devolvieron fuentes (prueba con otra pregunta o ajusta parÃ¡metros).")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EXTRA: Descargas Ãºtiles
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.expander("â¬‡ï¸ Exportar texto del PDF (limpio)"):
    all_text = "\n\n".join([f"[PÃ¡g. {p}] {t}" for p, t in pages if t])
    st.download_button(
        "Descargar .txt",
        data=all_text.encode("utf-8"),
        file_name=f"{source_name.rsplit('.',1)[0]}_texto.txt",
        mime="text/plain",
        use_container_width=True,
    )

st.markdown("---")
st.caption("RAG con FAISS (fallback Chroma) Â· Embeddings OpenAI v3 Â· Citas por pÃ¡gina Â· Controles de chunking y bÃºsqueda")
